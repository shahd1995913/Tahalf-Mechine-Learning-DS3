{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q5.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOKK7BMXGYjwZld4SM7nG7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahd1995913/Tahalf-Mechine-Learning-DS3/blob/main/Tasks/Q5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Linke for KNN code ](https://medium.com/@avulurivenkatasaireddy/k-nearest-neighbors-and-implementation-on-iris-data-set-f5817dd33711)"
      ],
      "metadata": {
        "id": "PITix7_3NpFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XtrTfJGJVGh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import operator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import recall_score , precision_score , roc_auc_score ,roc_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #to remove unwanted warnings\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col=['sepal_length','sepal_width','petal_length','petal_width','type']\n",
        "iris=pd.read_csv(\"/content/irasdataset.csv\",names=col)\n",
        "print(\"First five rows\")\n",
        "print(iris.head())\n",
        "print(\"*********\")\n",
        "print(\"columns\",iris.columns)\n",
        "print(\"*********\")\n",
        "print(\"shape:\",iris.shape)\n",
        "print(\"*********\")\n",
        "print(\"Size:\",iris.size)\n",
        "print(\"*********\")\n",
        "print(\"no of samples available for each type\")\n",
        "print(iris['type'].value_counts())\n",
        "print(\"*********\")\n",
        "print(iris.describe())"
      ],
      "metadata": {
        "id": "rZHhhQA7N3ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for finding euclidian diastance and knn:\n",
        "def euclidianDistance(data1, data2, length):\n",
        "    distance = 0\n",
        "    for x in range(length):\n",
        "        distance += np.square(data1[x] - data2[x])\n",
        "       \n",
        "    return np.sqrt(distance)\n",
        "def knn(trainingSet, testInstance, k):\n",
        " \n",
        "    distances = {}\n",
        "    sort = {}\n",
        "    length = testInstance.shape[1]\n",
        "    print(length)\n",
        "    \n",
        "    \n",
        "    # Calculating euclidean distance between each row of training data and test data\n",
        "    for x in range(len(trainingSet)):\n",
        "        \n",
        "       \n",
        "        dist = euclidianDistance(testInstance, trainingSet.iloc[x], length)\n",
        "        distances[x] = dist[0]\n",
        "       \n",
        " \n",
        "    \n",
        "    # Sorting them on the basis of distance\n",
        "    sorted_d = sorted(distances.items(), key=operator.itemgetter(1)) #by using it we store indices also\n",
        "    sorted_d1 = sorted(distances.items())\n",
        "    print(sorted_d[:5])\n",
        "    print(sorted_d1[:5])\n",
        "   \n",
        " \n",
        "    neighbors = []\n",
        "    \n",
        "    \n",
        "    # Extracting top k neighbors\n",
        "    for x in range(k):\n",
        "        neighbors.append(sorted_d[x][0])\n",
        "        counts = {\"Iris-setosa\":0,\"Iris-versicolor\":0,\"Iris-virginica\":0}\n",
        "    \n",
        "    \n",
        "    # Calculating the most freq class in the neighbors\n",
        "    for x in range(len(neighbors)):\n",
        "        response = trainingSet.iloc[neighbors[x]][-1]\n",
        " \n",
        "        if response in counts:\n",
        "            counts[response] += 1\n",
        "        else:\n",
        "            counts[response] = 1\n",
        "  \n",
        "    print(counts)\n",
        "    sortedVotes = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    print(sortedVotes)\n",
        "    return(sortedVotes[0][0], neighbors)"
      ],
      "metadata": {
        "id": "V0oqM33aL3CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testSet = [[1.4, 3.6, 3.4, 1.2]]\n",
        "test = pd.DataFrame(testSet)\n",
        "result,neigh = knn(iris, test, 4)#here we gave k=4\n",
        "print(\"And the flower is:\",result)\n",
        "print(\"the neighbors are:\",neigh)"
      ],
      "metadata": {
        "id": "DXW5jAsHJZ4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#divinding the data:\n",
        "x=iris.iloc[:,:4] #all parameters\n",
        "y=iris[\"type\"] #class labels\n",
        "#Training the model:\n",
        "neigh=KNeighborsClassifier(n_neighbors=4)\n",
        "neigh.fit(iris.iloc[:,:4],iris[\"type\"])"
      ],
      "metadata": {
        "id": "v8pF34f0M5sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#performing some test operation the model:\n",
        "testSet = [[1.4, 3.6, 3.4, 1.2]]\n",
        "test = pd.DataFrame(testSet)\n",
        "print(test)\n",
        "print(\"predicted:\",neigh.predict(test))\n",
        "print(\"neighbors\",neigh.kneighbors(test))"
      ],
      "metadata": {
        "id": "7urMlCxNM-_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=iris.iloc[1:,:3]#features\n",
        "y=iris.iloc[1:,4:]#class labels\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
        "#test_size determines the percentage of test data you want here\n",
        "#train=80% and test=20% data is randomly split"
      ],
      "metadata": {
        "id": "AY4UELmFNIbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_scores = []\n",
        "neighbors = list(np.arange(3,50,2))\n",
        "for n in neighbors:\n",
        "    knn = KNeighborsClassifier(n_neighbors = n,algorithm = 'brute')\n",
        "    \n",
        "    cross_val = cross_val_score(knn,x_train,y_train,cv = 5 , scoring = 'accuracy')\n",
        "    cv_scores.append(cross_val.mean())\n",
        "    \n",
        "error = [1-x for x in cv_scores]\n",
        "optimal_n = neighbors[ error.index(min(error)) ]\n",
        "knn_optimal = KNeighborsClassifier(n_neighbors = optimal_n,algorithm = 'brute')\n",
        "knn_optimal.fit(x_train,y_train)\n",
        "pred = knn_optimal.predict(x_test)\n",
        "acc = accuracy_score(y_test,pred)*100\n",
        "print(\"The accuracy for optimal k = {0} using brute is {1}\".format(optimal_n,acc))"
      ],
      "metadata": {
        "id": "jPEgP26_NId2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"classification_report using brute force\")\n",
        "print(classification_report(y_test,pred))"
      ],
      "metadata": {
        "id": "eTjhrNKYNIgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SVC(kernel = 'linear').fit(x_train,y_train)\n",
        "clf.predict(x_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "# Creates a confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "# Transform to df for easier plotting\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index = ['setosa','versicolor','virginica'], \n",
        "                     columns = ['setosa','versicolor','virginica'])\n",
        "\n",
        "sns.heatmap(cm_df, annot=True)\n",
        "plt.title('Accuracy using brute:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RoVmV0r_NIjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Problem 3: Naive Bayes\n",
        "---\n",
        "\n",
        "- Build a gaussian naive bayes model on [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris) , you can load the data using sklearn , split the data to train 70% and test 30%.\n",
        "\n",
        "[Code](https://towardsdatascience.com/machine-learning-basics-naive-bayes-classification-964af6f2a965)"
      ],
      "metadata": {
        "id": "zt18xOHsOqOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col=['sepal_length','sepal_width','petal_length','petal_width','type']\n",
        "dataset=pd.read_csv(\"/content/irasdataset.csv\",names=col)\n",
        "Naive_X = dataset.iloc[:,:4].values\n",
        "Naive_y = dataset['type'].values\n",
        "dataset.head(5)"
      ],
      "metadata": {
        "id": "kednpTztNzDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data to train 70% and test 30%.\n",
        "from sklearn.model_selection import train_test_split\n",
        "Naive_X_train, Naive_X_test, Naive_y_train, Naive_y_test = train_test_split(Naive_X, Naive_y, test_size = 0.3)"
      ],
      "metadata": {
        "id": "-BCZDxRtO76M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "Naive_X_train = sc.fit_transform(Naive_X_train)\n",
        "Naive_X_test = sc.transform(Naive_X_test)"
      ],
      "metadata": {
        "id": "pY1i-fDaPFaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(Naive_X_train, Naive_y_train)"
      ],
      "metadata": {
        "id": "W5JVvUtLPIW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Naive_y_pred = classifier.predict(Naive_X_test) \n",
        "Naive_y_pred"
      ],
      "metadata": {
        "id": "Y7IfZMe7PLOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm1 = confusion_matrix(Naive_y_test, Naive_y_pred)\n",
        "from sklearn.metrics import accuracy_score \n",
        "print (\"Accuracy : \", accuracy_score(Naive_y_test, Naive_y_pred))\n",
        "print(cm1)\n",
        "cm_df = pd.DataFrame(cm1,\n",
        "                     index = ['setosa','versicolor','virginica'], \n",
        "                     columns = ['setosa','versicolor','virginica'])\n",
        "\n",
        "sns.heatmap(cm_df, annot=True)\n",
        "plt.title('Accuracy using Naive Bayes:{0:.3f}'.format(accuracy_score(Naive_y_test, Naive_y_pred)))\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VhA8k8LMPO_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfNaive_ = pd.DataFrame({'Real Values':Naive_y_test, 'Predicted Values':Naive_y_pred})\n",
        "dfNaive_\n"
      ],
      "metadata": {
        "id": "JC7jaVh3PYX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# - If we have a discrete data which type of naive bayes should we use?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cMfjYW77RIFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sol --> Bernoulli Naive Bayes\n",
        "### if you have discrete features in 1s and 0s that represent the presence or absence of a feature.\n",
        "###  In that case, the features will be binary and we will use Bernoulli Naive Bayes. "
      ],
      "metadata": {
        "id": "-SVZp9hlRh-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# - Create a dataset with 0 and 1 ,the training data will have 1000 instance and each instance will have 8 features , the testing data will have 100 instance and build a naive bayes model on it \n",
        "---"
      ],
      "metadata": {
        "id": "c6-W5vDpRJHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "height = 1000\n",
        "\n",
        "df_0 = pd.DataFrame( index=range(height))\n",
        "df_0[\"1\"]=0\n",
        "df_0[\"2\"]=1\n",
        "df_0[\"3\"]=0\n",
        "df_0[\"4\"]=1\n",
        "df_0[\"5\"]=0\n",
        "df_0[\"6\"]=1\n",
        "df_0[\"7\"]=0\n",
        "df_0[\"8\"]=1\n",
        "df_0"
      ],
      "metadata": {
        "id": "aQXw87OAWjes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB \n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#creating dataset with with 0 and 1\n",
        "X = np.random.randint(2, size=(1000,8))\n",
        "Y = np.random.randint(2, size=(1000, 1))"
      ],
      "metadata": {
        "id": "RIIeXJJsYe0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X[:100, :8]\n",
        "y_test = Y[:100, :1]\n",
        "y_test"
      ],
      "metadata": {
        "id": "PphOlAUAYlw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = BernoulliNB()\n",
        "\n",
        "model = clf.fit(X, Y)"
      ],
      "metadata": {
        "id": "KsHwiwDcY2dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred =clf.predict(X_test)\n",
        "acc_score = accuracy_score(y_test, y_pred)\n",
        "print(acc_score)"
      ],
      "metadata": {
        "id": "4YJDXOVHY5RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=\"r\", s=50, cmap='RdBu')\n",
        "l = plt.axis()\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, s=20, cmap='RdBu', alpha=0.1)\n",
        "plt.axis(l);"
      ],
      "metadata": {
        "id": "6kFDLSnZY_U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB"
      ],
      "metadata": {
        "id": "uLtP98grbHNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randint(2, size=(1000, 8))\n",
        "\n",
        "y = np.random.randint(2, size=(1000, 1)).ravel()"
      ],
      "metadata": {
        "id": "e3njz7vHeTxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:10]"
      ],
      "metadata": {
        "id": "evBxCTF3eT0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /* Create Bernoulli Naive Bayes object with prior probabilities of each class */\n",
        "clf = BernoulliNB(class_prior=[0.25, 0.5])\n",
        "\n",
        "# /* Train model */\n",
        "model = clf.fit(X, y)"
      ],
      "metadata": {
        "id": "7C5NRchGeT3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bernoul_X_test = X[:100, :8]\n",
        "Bernoul_y_test = Y[:100, :1]\n",
        "\n",
        "Bernoul_y_pred =clf.predict(Bernoul_X_test)\n",
        "acc_score = accuracy_score(Bernoul_y_test, Bernoul_y_pred)\n",
        "print(acc_score)"
      ],
      "metadata": {
        "id": "07e6ANmmbHQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}