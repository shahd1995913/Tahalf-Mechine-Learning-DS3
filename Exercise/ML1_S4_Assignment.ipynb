{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahd1995913/Tahalf-Mechine-Learning-DS3/blob/main/Exercise/ML1_S4_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096071b3",
      "metadata": {
        "id": "096071b3"
      },
      "source": [
        "# ML1-S4 (Assignment)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a7419d1",
      "metadata": {
        "id": "4a7419d1"
      },
      "source": [
        "## Problem 1:  Random Forest Regression\n",
        "---\n",
        "\n",
        "- What is the Difference between Random Forests and Decision tree?\n",
        "\n",
        "- When to use random forest ?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **When to use random forest ?**\n",
        "1. Random forest algorithm can be used for both classifications and regression task.\n",
        "2. It typically provides very high accuracy.\n",
        "3.  Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data.\n",
        "4. If there are more trees, it usually won’t allow overfitting trees in the model.\n",
        "5. It has the power to handle a large data set with high dimensionality\n",
        "6. Random forests add ensemble learning to the mix, making decision trees even more robust and especially well equipped to deal with noisy data, whereas standard regression methods can get easily confused by noise.\n",
        "7. One of the biggest advantages of random forest is its versatility. It can be used for both regression and classification tasks, and it’s also easy to view the relative importance it assigns to the input features.\n",
        "\n",
        "8. Random forest is also a very handy algorithm because the default hyperparameters it uses often produce a good prediction result. Understanding the hyperparameters is pretty straightforward, and there's also not that many of them. \n",
        "\n",
        "9. One of the biggest problems in machine learning is overfitting, but most of the time this won’t happen thanks to the random forest classifier. If there are enough trees in the forest, the classifier won’t overfit the model."
      ],
      "metadata": {
        "id": "s3pgXsTDZNwe"
      },
      "id": "s3pgXsTDZNwe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# When to use to decision tree:\n",
        "\n",
        "1. When model to be simple and explainable\n",
        "2.  non parametric model\n",
        "3. When you don't want to worry about feature selection or regularization or worry about multi-collinearity.\n",
        "4. You can overfit the tree and build a model if you are sure of validation or test data set is going to be subset of training data set or almost overlapping instead of unexpected.\n",
        "5.  A decision tree is built on an entire dataset, using all the features/variables of interest\n",
        "6.  A decision tree combines some decisions.\n",
        "# Advantages and Disadvantages of Decision Tree\n",
        "## Advantages\n",
        "-  Easy\n",
        "- Transparent process\n",
        "- Handle both numerical and categorical data\n",
        "- Larger the data, the better the result\n",
        "- Speed \n",
        "## Disadvantages\n",
        "- May overfit\n",
        "- Pruning process large\n",
        "- Optimization unguaranteed\n",
        "- Complex calculations\n",
        "- Deflection high\n",
        "\n",
        "# Bagging is the process of establishing random forests while decisions work parallelly.\n",
        "\n",
        "1. Bagging\n",
        "Take some training data set\n",
        "Make a decision tree\n",
        "Repeat the process for a definite period\n",
        "Now take the major vote. The one that wins is your decision to take.\n",
        "2. Bootstrapping\n",
        "Bootstrapping is randomly choosing samples from training data. This is a random procedure. \n",
        "\n",
        "# When to use random forest :\n",
        "\n",
        "1. When you don't bother much about interpreting the model but want better accuracy.\n",
        "2. random forest combines several decision trees.\n",
        "2. Random forest will reduce variance part of error rather than bias part, so on a given training data set decision tree may be more accurate than a random forest. But on an unexpected validation data set, Random forest always wins in terms of accuracy.\n",
        "3. Random Forest is essentially a collection of Decision Trees.\n",
        "4.  random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results. After a large number of trees are built using this method, each tree \"votes\" or chooses the class, and the class receiving the most votes by a simple majority is the \"winner\" or predicted class.\n",
        "\n",
        "# Advantages\n",
        "- Powerful and highly accurate\n",
        "- No need to normalizing\n",
        "- Can handle several features at once\n",
        "- Run trees in parallel ways\n",
        "# Disadvantages\n",
        "- They are biased to certain features sometimes\n",
        "- Slow\n",
        "- Can not be used for linear methods\n",
        "- Worse for high dimensional data\n",
        "\n",
        "**Decision trees are very easy as compared to the random forest. A decision tree combines some decisions, whereas a random forest combines several decision trees. Thus, it is a long process, yet slow.**\n",
        "\n",
        "**Whereas, a decision tree is fast and operates easily on large data sets, especially the linear one. The random forest model needs rigorous training. When you are trying to put up a project, you might need more than one model. Thus, a large number of random forests, more the time.**\n",
        "\n",
        "**It depends on your requirements. If you have less time to work on a model, you are bound to choose a decision tree. However, stability and reliable predictions are in the basket of random forests.**"
      ],
      "metadata": {
        "id": "5aFT31azW5Jn"
      },
      "id": "5aFT31azW5Jn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree (DT) is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.\n",
        "## The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. \n",
        "## This flowchart-like structure helps  in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret."
      ],
      "metadata": {
        "id": "VfWVq-fJVNgU"
      },
      "id": "VfWVq-fJVNgU"
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Idea behind any DT  algorithm is as follows:\n",
        "1. Select the best attribute using Attribute Selection Measures(ASM) to\n",
        "split the records.\n",
        "2. Make that attribute a decision node and breaks the dataset into\n",
        "smaller subsets.\n",
        "3. Starts tree building by repeating this process recursively for each\n",
        "child until one of the condition will match:\n",
        "- All the tuples belong to the same attribute value.\n",
        "- There are no more remaining attributes.\n",
        "- There are no more instances"
      ],
      "metadata": {
        "id": "1Bv8cp-qVnbt"
      },
      "id": "1Bv8cp-qVnbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. DT are easy to interpret and visualize.\n",
        "2. It can easily capture Non-linear patterns.\n",
        "3. It requires fewer data preprocessing from the user, for\n",
        "example, there is no need to normalize columns.\n",
        "4. It can be used for feature engineering such as predicting\n",
        "missing values, suitable for variable selection.\n",
        "5. The DT has no assumptions about distribution\n",
        "because of the non-parametric nature of the algorithm.\n",
        "6. DT is a white box type of ML algorithm. It shares\n",
        "internal decision-making logic, which is not available in the\n",
        "black box type of algorithms such as Neural Network.\n",
        "7. Its training time is faster compared to the neural network\n",
        "algorithm.\n",
        "8. The time complexity of DT is a function of the\n",
        "number of records and number of attributes in the given data.\n",
        "9. The DT is a distribution-free or non-parametric\n",
        "method, which does not depend upon probability distribution\n",
        "assumptions.\n",
        "10. DT can handle high dimensional data with good\n",
        "accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "-b7ayXAEWNtD"
      },
      "id": "-b7ayXAEWNtD"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "ML1_S4_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}